{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p.kuznetsov/presto/venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import webdataset as wds\n",
    "from presto.dataops.dataset import (\n",
    "    TAR_BUCKET,\n",
    "    Dataset,\n",
    "    S1_S2_ERA5_SRTM_DynamicWorldMonthly_2020_2021,\n",
    ")\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from typing import List, Tuple, Optional, Dict, Any, Iterable\n",
    "from collections import OrderedDict\n",
    "from typing import OrderedDict as OrderedDictType\n",
    "import numpy as np\n",
    "from presto.dataops import MASK_STRATEGIES, plot_masked\n",
    "from presto.utils import (\n",
    "    DEFAULT_SEED,\n",
    "    config_dir,\n",
    "    device,\n",
    "    initialize_logging,\n",
    "    seed_everything,\n",
    "    timestamp_dirname,\n",
    "    update_data_dir,\n",
    ")\n",
    "import random\n",
    "from random import choice, randint, sample\n",
    "from presto.dataops.pipelines.dynamicworld import DynamicWorld2020_2021\n",
    "from presto.dataops.pipelines.s1_s2_era5_srtm import (\n",
    "    NORMED_BANDS,\n",
    ")\n",
    "NUM_TIMESTEPS = 60\n",
    "TIMESTEPS_IDX = list(range(NUM_TIMESTEPS))\n",
    "from collections import namedtuple\n",
    "from dataclasses import dataclass\n",
    "from presto.dataops.utils import construct_single_presto_input\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "MASK_STRATEGIES = (\n",
    "    \"group_bands\",\n",
    "    \"random_timesteps\",\n",
    "    \"chunk_timesteps\",\n",
    "    \"random_combinations\",\n",
    ")\n",
    "NUM_TIMESTEPS = 60\n",
    "S1_BANDS = [\"VV\", \"VH\"]\n",
    "ERA5_BANDS = [\"temperature_2m\", \"total_precipitation\"]\n",
    "SRTM_BANDS = [\"elevation\", \"slope\"]\n",
    "BANDS_GROUPS_IDX: OrderedDictType[str, List[int]] = OrderedDict(\n",
    "    {\n",
    "        \"S1\": [NORMED_BANDS.index(b) for b in S1_BANDS],\n",
    "        \"S2_RGB\": [NORMED_BANDS.index(b) for b in [\"B2\", \"B3\", \"B4\"]],\n",
    "        \"S2_Red_Edge\": [NORMED_BANDS.index(b) for b in [\"B5\", \"B6\", \"B7\"]],\n",
    "        \"S2_NIR_10m\": [NORMED_BANDS.index(b) for b in [\"B8\"]],\n",
    "        \"S2_NIR_20m\": [NORMED_BANDS.index(b) for b in [\"B8A\"]],\n",
    "        \"S2_SWIR\": [NORMED_BANDS.index(b) for b in [\"B11\", \"B12\"]],  # Include B10?\n",
    "        \"ERA5\": [NORMED_BANDS.index(b) for b in ERA5_BANDS],\n",
    "        \"SRTM\": [NORMED_BANDS.index(b) for b in SRTM_BANDS],\n",
    "        \"NDVI\": [NORMED_BANDS.index(\"NDVI\")],\n",
    "    }\n",
    ")\n",
    "\n",
    "BAND_EXPANSION = [len(x) for x in BANDS_GROUPS_IDX.values()]\n",
    "SRTM_INDEX = list(BANDS_GROUPS_IDX.keys()).index(\"SRTM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mask(strategy: str, mask_ratio: float) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Make a mask for a given strategy and percentage of masked values.\n",
    "    Args:\n",
    "        strategy: The masking strategy to use. One of MASK_STRATEGIES\n",
    "        mask_ratio: The percentage of values to mask. Between 0 and 1.\n",
    "    \"\"\"\n",
    "\n",
    "    # SRTM is included here, but ignored by Presto\n",
    "    mask = np.full((NUM_TIMESTEPS, len(BANDS_GROUPS_IDX)), False)\n",
    "    dw_mask = np.full(NUM_TIMESTEPS, False)\n",
    "    srtm_mask = False\n",
    "    num_tokens_to_mask = int(((NUM_TIMESTEPS * len(BANDS_GROUPS_IDX)) + 1) * mask_ratio)\n",
    "\n",
    "    def mask_topography(srtm_mask, num_tokens_to_mask, mask_ratio):\n",
    "        should_flip = random.random() < mask_ratio\n",
    "        if should_flip:\n",
    "            srtm_mask = True\n",
    "            num_tokens_to_mask -= 1\n",
    "        return srtm_mask, num_tokens_to_mask\n",
    "\n",
    "    def random_masking(mask, dw_mask, num_tokens_to_mask: int):\n",
    "        if num_tokens_to_mask > 0:\n",
    "            # we set SRTM to be True - this way, it won't get randomly assigned.\n",
    "            # at the end of the function, it gets properly assigned\n",
    "            mask[:, SRTM_INDEX] = True\n",
    "            # then, we flatten the mask and dw arrays\n",
    "            all_tokens_mask = np.concatenate([dw_mask, mask.flatten()])\n",
    "            unmasked_tokens = all_tokens_mask == False\n",
    "            idx = np.flatnonzero(unmasked_tokens)\n",
    "            np.random.shuffle(idx)\n",
    "            idx = idx[:num_tokens_to_mask]\n",
    "            all_tokens_mask[idx] = True\n",
    "            mask = all_tokens_mask[NUM_TIMESTEPS:].reshape((NUM_TIMESTEPS, len(BANDS_GROUPS_IDX)))\n",
    "            dw_mask = all_tokens_mask[:NUM_TIMESTEPS]\n",
    "        return mask, dw_mask\n",
    "\n",
    "    # RANDOM BANDS\n",
    "    if strategy == \"random_combinations\":\n",
    "        srtm_mask, num_tokens_to_mask = mask_topography(srtm_mask, num_tokens_to_mask, mask_ratio)\n",
    "        mask, dw_mask = random_masking(mask, dw_mask, num_tokens_to_mask)\n",
    "\n",
    "    elif strategy == \"group_bands\":\n",
    "        srtm_mask, num_tokens_to_mask = mask_topography(srtm_mask, num_tokens_to_mask, mask_ratio)\n",
    "        # next, we figure out how many tokens we can mask\n",
    "        num_band_groups_to_mask = int(num_tokens_to_mask / NUM_TIMESTEPS)\n",
    "        num_tokens_to_mask -= NUM_TIMESTEPS * num_band_groups_to_mask\n",
    "        assert num_tokens_to_mask >= 0\n",
    "        # tuple because of mypy, which thinks lists can only hold one type\n",
    "        band_groups: List[Any] = list(range(len(BANDS_GROUPS_IDX))) + [\"DW\"]\n",
    "        band_groups.remove(SRTM_INDEX)\n",
    "        band_groups_to_mask = sample(band_groups, num_band_groups_to_mask)\n",
    "        for band_group in band_groups_to_mask:\n",
    "            if band_group == \"DW\":\n",
    "                dw_mask[:] = True\n",
    "            else:\n",
    "                mask[:, band_group] = True\n",
    "        mask, dw_mask = random_masking(mask, dw_mask, num_tokens_to_mask)\n",
    "\n",
    "    # RANDOM TIMESTEPS\n",
    "    elif strategy == \"random_timesteps\":\n",
    "        srtm_mask, num_tokens_to_mask = mask_topography(srtm_mask, num_tokens_to_mask, mask_ratio)\n",
    "        # +1 for dynamic world, -1 for the SRTM\n",
    "        timesteps_to_mask = int(num_tokens_to_mask / (len(BANDS_GROUPS_IDX)))\n",
    "        num_tokens_to_mask -= (len(BANDS_GROUPS_IDX)) * timesteps_to_mask\n",
    "        timesteps = sample(TIMESTEPS_IDX, k=timesteps_to_mask)\n",
    "        mask[timesteps] = True\n",
    "        dw_mask[timesteps] = True\n",
    "        mask, dw_mask = random_masking(mask, dw_mask, num_tokens_to_mask)\n",
    "    elif strategy == \"chunk_timesteps\":\n",
    "        srtm_mask, num_tokens_to_mask = mask_topography(srtm_mask, num_tokens_to_mask, mask_ratio)\n",
    "        timesteps_to_mask = int(num_tokens_to_mask / (len(BANDS_GROUPS_IDX)))\n",
    "        num_tokens_to_mask -= (len(BANDS_GROUPS_IDX)) * timesteps_to_mask\n",
    "        start_idx = randint(0, NUM_TIMESTEPS - timesteps_to_mask)\n",
    "        mask[start_idx : start_idx + timesteps_to_mask] = True  # noqa\n",
    "        dw_mask[start_idx : start_idx + timesteps_to_mask] = True  # noqa\n",
    "        mask, dw_mask = random_masking(mask, dw_mask, num_tokens_to_mask)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown strategy {strategy} not in {MASK_STRATEGIES}\")\n",
    "\n",
    "    mask[:, SRTM_INDEX] = srtm_mask\n",
    "    return np.repeat(mask, BAND_EXPANSION, axis=1), dw_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MaskParams:\n",
    "    strategies: Tuple[str, ...] = (\"NDVI\",)\n",
    "    ratio: float = 0.5\n",
    "\n",
    "    def __post_init__(self):\n",
    "        for strategy in self.strategies:\n",
    "            assert strategy in [\n",
    "                \"group_bands\",\n",
    "                \"random_timesteps\",\n",
    "                \"chunk_timesteps\",\n",
    "                \"random_combinations\",\n",
    "            ]\n",
    "\n",
    "    def mask_data(self, eo_data: np.ndarray, dw_data: np.ndarray):\n",
    "        strategy = choice(self.strategies)\n",
    "        mask, dw_mask = make_mask(strategy=strategy, mask_ratio=self.ratio)\n",
    "        x = eo_data * ~mask\n",
    "        y = np.zeros(eo_data.shape).astype(np.float32)\n",
    "        y[mask] = eo_data[mask]\n",
    "\n",
    "        masked_dw_tokens = np.ones_like(dw_data) * DynamicWorld2020_2021.missing_data_class\n",
    "        x_dw = np.where(dw_mask, masked_dw_tokens, dw_data)\n",
    "        y_dw = np.zeros(x_dw.shape).astype(np.int16)\n",
    "        y_dw[dw_mask] = dw_data[dw_mask]\n",
    "\n",
    "        return mask, dw_mask, x, y, x_dw, y_dw, strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from your_module import MaskParams, construct_single_presto_input  # Replace with actual imports\n",
    "\n",
    "class FranceCropsFullDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: str,\n",
    "        split: str,\n",
    "        mask_params: MaskParams,\n",
    "        shuffle: bool = True,\n",
    "        seed: int = 42,\n",
    "        preprocess: bool = True,\n",
    "        cache_dir: Optional[str] = None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.mask_params = mask_params\n",
    "        self.split = split\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if preprocess:\n",
    "            if cache_dir is not None and os.path.exists(cache_dir):\n",
    "                # Load metadata and check compatibility\n",
    "                metadata_path = os.path.join(cache_dir, 'metadata.json')\n",
    "                if not os.path.exists(metadata_path):\n",
    "                    raise ValueError(f\"Metadata not found in {cache_dir}\")\n",
    "                with open(metadata_path, 'r') as f:\n",
    "                    saved_metadata = json.load(f)\n",
    "                # Generate current metadata for comparison\n",
    "                current_metadata = self._get_metadata(dataset, split, mask_params, shuffle, seed)\n",
    "                if saved_metadata != current_metadata:\n",
    "                    raise ValueError(\"Cache parameters do not match current parameters.\")\n",
    "                # Load the preprocessed dataset\n",
    "                self.base_dataset = load_dataset(os.path.join(cache_dir, 'dataset'))\n",
    "            else:\n",
    "                # Preprocess the dataset\n",
    "                self.base_dataset = self._load_dataset(dataset)\n",
    "                self.base_dataset = self._preprocess()\n",
    "                # Save to cache if directory provided\n",
    "                if cache_dir is not None:\n",
    "                    os.makedirs(cache_dir, exist_ok=True)\n",
    "                    # Save the dataset\n",
    "                    dataset_path = os.path.join(cache_dir, 'dataset')\n",
    "                    self.base_dataset.save_to_disk(dataset_path)\n",
    "                    # Save metadata\n",
    "                    metadata = self._get_metadata(dataset, split, mask_params, shuffle, seed)\n",
    "                    metadata_path = os.path.join(cache_dir, 'metadata.json')\n",
    "                    with open(metadata_path, 'w') as f:\n",
    "                        json.dump(metadata, f, indent=4)\n",
    "        else:\n",
    "            # Load without preprocessing\n",
    "            self.base_dataset = self._load_dataset(dataset)\n",
    "        \n",
    "        # Ensure the dataset is in PyTorch format\n",
    "        self.base_dataset.set_format(type='torch')\n",
    "\n",
    "    def _get_metadata(self, dataset: str, split: str, mask_params: MaskParams, shuffle: bool, seed: int) -> dict:\n",
    "        \"\"\"Generate metadata dictionary for parameter compatibility checks.\"\"\"\n",
    "        return {\n",
    "            'dataset': dataset,\n",
    "            'split': split,\n",
    "            'mask_params': mask_params.__dict__,\n",
    "            'shuffle': shuffle,\n",
    "            'seed': seed,\n",
    "        }\n",
    "\n",
    "    def _load_dataset(self, dataset: str) -> Dataset:\n",
    "        \"\"\"Load the base dataset from Hugging Face.\"\"\"\n",
    "        return load_dataset(dataset, split=self.split)\n",
    "\n",
    "    def _expand_function(self, examples):\n",
    "        \"\"\"Expand the time series data into individual slices.\"\"\"\n",
    "        new_x = [slice for x_array in examples['x'] for slice in x_array]\n",
    "        new_y = [y_label for y_label, x_array in zip(examples['y'], examples['x']) \n",
    "                 for _ in range(len(x_array))]\n",
    "        return {\"x\": new_x, \"y\": new_y}\n",
    "\n",
    "    def _convert_to_presto(self, examples):\n",
    "        \"\"\"Convert examples to Presto input format.\"\"\"\n",
    "        x_tensor = torch.tensor(examples['x'], dtype=torch.float32)\n",
    "        bands = [\"B1\", \"B2\", \"B3\", \"B4\", \"B5\", \"B6\", \"B7\", \"B8\", \"B8A\", \"B9\", \"B11\", \"B12\"]\n",
    "        presto_input, mask, dynamic = construct_single_presto_input(s2=x_tensor, s2_bands=bands)\n",
    "        latlons = np.zeros(2, dtype=np.float32)\n",
    "        start_month = 0\n",
    "\n",
    "        mask, mask_dw, x, y, x_dw, y_dw, strat = self.mask_params.mask_data(presto_input, dynamic)\n",
    "\n",
    "        return {\n",
    "            \"x\": x, \"y\": y, \"mask\": mask, \"start_month\": start_month,\n",
    "            \"latlons\": latlons, \"mask_dw\": mask_dw, \"x_dw\": x_dw, \n",
    "            \"y_dw\": y_dw, \"strategy\": strat\n",
    "        }\n",
    "\n",
    "    def _preprocess(self) -> Dataset:\n",
    "        \"\"\"Apply preprocessing steps including expansion and conversion.\"\"\"\n",
    "        # Expand the dataset\n",
    "        expanded_dataset = self.base_dataset.map(\n",
    "            self._expand_function,\n",
    "            batched=True,\n",
    "            remove_columns=[\"x\", \"y\"],\n",
    "        )\n",
    "        # Shuffle if required\n",
    "        if self.shuffle:\n",
    "            expanded_dataset = expanded_dataset.shuffle(seed=self.seed)\n",
    "        # Convert to Presto format\n",
    "        processed_dataset = expanded_dataset.map(self._convert_to_presto)\n",
    "        return processed_dataset\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.base_dataset)\n",
    "\n",
    "    def __getitem__(self, idx) -> dict:\n",
    "        return self.base_dataset[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████| 20000/20000 [04:43<00:00, 70.51 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████| 2000000/2000000 [41:59<00:00, 793.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "mask_params = MaskParams(MASK_STRATEGIES, 0.75)\n",
    "france_crops_dataset = FranceCropsFullDataset(\n",
    "    dataset=\"saget-antoine/francecrops\",\n",
    "    split=\"train\",\n",
    "    mask_params=mask_params,\n",
    "    shuffle=True,\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"france_crops_full_dataset.pkl\", \"wb\") as f:\n",
    "    pickle.dump(france_crops_dataset, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    france_crops_dataset, batch_size=32, shuffle=True, num_workers=4, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch_step, b in enumerate(train_dataloader):\n",
    "    mask, x, y, start_month = b[\"mask\"].to(device), b[\"x\"].to(device), b[\"y\"].to(device), b[\"start_month\"]\n",
    "    dw_mask, x_dw, y_dw = b[\"mask_dw\"].to(device), b[\"x_dw\"].to(device).long(), b[\"y_dw\"].to(device).long()\n",
    "    latlons = b[\"latlons\"].to(device)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
